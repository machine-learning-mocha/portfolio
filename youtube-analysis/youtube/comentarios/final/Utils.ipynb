{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath, delim=',', top_words=None, pad_len=None):\n",
    "    df = pd.read_csv(filepath, delimiter=delim)\n",
    "    \n",
    "    comments = df['text'].tolist()\n",
    "    sentiment = df['sentiment'].tolist()\n",
    "    file_name = \"word_to_id.npy\"\n",
    "    file_saved = Path(file_name)\n",
    "    if file_saved.is_file():\n",
    "        word_to_id = np.load('word_to_id.npy').tolist()\n",
    "    else: \n",
    "        # Word frequency\n",
    "        word_freq = Counter() \n",
    "        for comment in comments:\n",
    "            for word in str(comment).split():\n",
    "                if not word in word_freq:\n",
    "                    word_freq[word] = 0\n",
    "                word_freq[word] += 1\n",
    "        word_to_id = {}\n",
    "        if top_words:\n",
    "            top = top_words\n",
    "            most_common_words = word_freq.most_common(top_words)\n",
    "            print(len(most_common_words))\n",
    "            print(len(range(top_words)))\n",
    "            for i in range(top_words):\n",
    "                word_to_id[most_common_words[i][0]] = top\n",
    "                top -= 1\n",
    "        else:\n",
    "            top = len(word_freq)\n",
    "            most_common_words = word_freq.most_common()\n",
    "\n",
    "            for i in range(len(word_freq)):\n",
    "                word_to_id[most_common_words[i][0]] = top\n",
    "                top -= 1\n",
    "\n",
    "        np.save(file_name, word_to_id)\n",
    "    \n",
    "    # Convert comments\n",
    "    max_len = 0\n",
    "    \n",
    "    X_data = []\n",
    "    for comment in comments:\n",
    "        # Get max comment length\n",
    "        if len(str(comment).split()) > max_len:\n",
    "            max_len = len(str(comment).split())\n",
    "        \n",
    "        aux = []\n",
    "        for word in str(comment).split():\n",
    "            if not word in word_to_id:\n",
    "                aux.append(0)\n",
    "            else:\n",
    "                aux.append(word_to_id[word])\n",
    "        \n",
    "        X_data.append(aux)\n",
    "    \n",
    "    padding_size = 0\n",
    "    if pad_len:\n",
    "        padding_size = pad_len\n",
    "    else:\n",
    "        padding_size = max_len\n",
    "    return pad_sequences(X_data, padding_size, padding='post'), to_categorical([(s + 1) / 2 for s in sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_comment(comment, word_to_id, pad_len=None):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    comment = \" \".join(re.split(\"[^a-zA-Z]*\", comment))\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    aux = []\n",
    "    for word in str(comment).split():\n",
    "        if not word in word_to_id:\n",
    "            aux.append(0)\n",
    "        else:\n",
    "            aux.append(word_to_id[word])\n",
    "    \n",
    "    data.append(aux)\n",
    "    \n",
    "    if pad_len:\n",
    "        data = pad_sequences(data, pad_len, padding = 'post')\n",
    "    else:\n",
    "        data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, train, test, validation, epochs, callbacks, file):\n",
    "    model_history = []\n",
    "    X_train, y_train = train\n",
    "    X_test, y_test = test\n",
    "    X_val, y_val = validation\n",
    "    \n",
    "    model_history.append(model.fit(X_train, y_train, epochs=10, validation_data=[X_val, y_val], callbacks=callbacks))\n",
    "    result = model.evaluate(X_test, y_test)\n",
    "\n",
    "    print('Accuracy (test set): {}'.format(result[1]))\n",
    "\n",
    "    plt.plot(model_history[-1].history['acc'])\n",
    "    plt.plot(model_history[-1].history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    model.save_weights(file + \".hdf5\")\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
