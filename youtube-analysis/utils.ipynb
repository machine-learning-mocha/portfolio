{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, delim='\\t'):\n",
    "    \n",
    "    return pd.read_csv(filepath, delimiter=delim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "\n",
    "    # df.Phrase.replace({r\"[^a-zA-Z' ]+\": ''}, regex=True, inplace=True)\n",
    "    # df.Phrase = df.Phrase.str.lower()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(reviews, sentiment):\n",
    "    \n",
    "    reviews = np.array(reviews)\n",
    "    sentiment = np.array(sentiment)\n",
    "    \n",
    "    cnt = Counter()\n",
    "    for i in sentiment:\n",
    "        cnt[i] += 1\n",
    "    \n",
    "    top_class, top_count = cnt.most_common()[0]\n",
    "    \n",
    "    balanced_rev = []\n",
    "    balanced_sent = []\n",
    "    for i in np.unique(sentiment):\n",
    "        if i == top_class:\n",
    "            balanced_rev.append(reviews[sentiment == i])\n",
    "            balanced_sent.append(sentiment[sentiment == i])\n",
    "        else:\n",
    "            balanced_rev.append(resample(reviews[sentiment == i], n_samples=top_count))\n",
    "            balanced_sent.append(resample(sentiment[sentiment == i], n_samples=top_count))\n",
    "        \n",
    "    balanced_rev = np.hstack(balanced_rev)\n",
    "    balanced_sent = np.hstack(balanced_sent)\n",
    "    \n",
    "    return list(balanced_rev), list(balanced_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(filepath, delim='\\t', balancing=True, top_words=None, pad_len=None):\n",
    "    \n",
    "    # Data Clean Up\n",
    "    df = read_data(filepath, delim)\n",
    "    # df = clean_text(df)\n",
    "    \n",
    "    reviews = df['text'].tolist()\n",
    "    sentiment = df['sentiment'].tolist()\n",
    "    #if balancing:\n",
    "    #    reviews, sentiment = balance_data(reviews, sentiment)\n",
    "    \n",
    "    # Remove exceding spaces and stopwords\n",
    "    # reviews = [' ' . join(r.split()) for r in reviews]\n",
    "    \n",
    "    #cleaned_reviews = []\n",
    "    #for r in reviews:\n",
    "    #    aux = []\n",
    "    #    for word in r.split():\n",
    "    #        if not word in ENGLISH_STOP_WORDS:\n",
    "    #            aux.append(word)\n",
    "    #    cleaned_reviews.append(' '.join(aux))\n",
    "    #\n",
    "    #reviews = cleaned_reviews\n",
    "    \n",
    "    # Word frequency\n",
    "    word_freq = Counter() \n",
    "    for review in reviews:\n",
    "        for word in str(review).split():\n",
    "            if not word in word_freq:\n",
    "                word_freq[word] = 0\n",
    "            word_freq[word] += 1\n",
    "    # print(word_freq)\n",
    "    # Map word to id\n",
    "    word_to_id = {}\n",
    "    if top_words:\n",
    "        top = top_words\n",
    "        most_common_words = word_freq.most_common(top_words)\n",
    "        print(len(most_common_words))\n",
    "        print(len(range(top_words)))\n",
    "        for i in range(top_words):\n",
    "            word_to_id[most_common_words[i][0]] = top\n",
    "            top -= 1\n",
    "    else:\n",
    "        top = len(word_freq)\n",
    "        most_common_words = word_freq.most_common()\n",
    "\n",
    "        for i in range(len(word_freq)):\n",
    "            word_to_id[most_common_words[i][0]] = top\n",
    "            top -= 1\n",
    "    \n",
    "    np.save(\"word_to_id.npy\", word_to_id)\n",
    "    \n",
    "    # Convert reviews\n",
    "    max_len = 0\n",
    "    \n",
    "    X_data = []\n",
    "    for review in reviews:\n",
    "        # Get max review length\n",
    "        if len(str(review).split()) > max_len:\n",
    "            max_len = len(str(review).split())\n",
    "        \n",
    "        aux = []\n",
    "        for word in str(review).split():\n",
    "            if not word in word_to_id:\n",
    "                aux.append(0)\n",
    "            else:\n",
    "                aux.append(word_to_id[word])\n",
    "        \n",
    "        X_data.append(aux)\n",
    "    \n",
    "    padding_size = 0\n",
    "    if pad_len:\n",
    "        padding_size = pad_len\n",
    "    else:\n",
    "        padding_size = max_len\n",
    "    print(to_categorical(sentiment))\n",
    "    return pad_sequences(X_data, padding_size, padding='post'), to_categorical([(s + 1) / 2 for s in sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_review(review, word_to_id, pad_len=None):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    review = \" \".join(re.split(\"[^a-zA-Z]*\", review))\n",
    "    review = review.lower()\n",
    "    \n",
    "    aux = []\n",
    "    for word in str(review).split():\n",
    "        if not word in word_to_id:\n",
    "            aux.append(0)\n",
    "        else:\n",
    "            aux.append(word_to_id[word])\n",
    "    \n",
    "    data.append(aux)\n",
    "    \n",
    "    if pad_len:\n",
    "        data = pad_sequences(data, pad_len, padding='post')\n",
    "    else:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
